# -*- coding: utf-8 -*-
"""Lab_8_Exercise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zgElZEsJ7WaQsxauGsVM9jqc9q0TerNW
"""

# ============================================================
#  IMPORT LIBRARIES
# ============================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

"""# **Remember: This lab has 30% of the D1 mark for the module.**

**You will be conducting an ML experiment from beginning to end for the Titanic dataset. You can download the dataset from Moodle [here](https://moodle.bcu.ac.uk/mod/folder/view.php?id=8931539&forceview=1)**

**1. You need to fill in the missing parts of the codes in the questions. You will need to write comments in the text cells.**

**2. You can get help on the code for Section 1 from [Lab 3 Exercise](https://moodle.bcu.ac.uk/mod/folder/view.php?id=8931484), Section 2 from [Lab 2 Exercise](https://moodle.bcu.ac.uk/mod/folder/view.php?id=8931483) and Section 3 from [Lab 7](https://moodle.bcu.ac.uk/mod/folder/view.php?id=9197859)**
"""

# ============================================================
# LOAD DATA
# ============================================================

df = pd.read_csv('/content/Titanic.csv')   # <-- Update path if running locally
df.head()

"""# **Section 1: Exploratory Data Analysis (EDA)**

<font color="red">**Question 1: Get basic information about the dataset by using the info and describe methods**</font>

**Write your comments on the data here:**
Type & Size: It’s a pandas.DataFrame with 891 rows and 9 columns.

Columns & Data Types:

6 integer columns: Survived, Pclass, Sex, SibSp, Parch, Embarked

3 float columns: Age, Fare, Fare_scaled

Missing Values: None — all columns are fully populated.

Summary Stats:

Survived: ~38% survived, binary (0/1)

Pclass: Passenger class, mostly 3rd class

Sex: Encoded as 0/1, mostly 1

Age: Mean ~29 years, range 0.42–80

SibSp & Parch: Small family sizes, mostly 0–1

Fare: Wide range, mean ~32

Embarked: Encoded as 0–2, mean ~1.54

Fare_scaled: Standardized version of Fare
"""

# ============================================================
# Get BASIC INFO about the data
# ============================================================
# Print a concise summary of the DataFrame including:
# - Number of non-null entries per column
# - Column names and data types
# - Memory usage
print(df.info())

# Print descriptive statistics for numerical columns including:
# - Count, mean, standard deviation, min, max
# - 25th, 50th (median), and 75th percentiles
print(df.describe())

"""<font color="red">**Question 2: Draw a Histograom to show the distribution of the Fare on the Titanic**</font>

**Write your coments on the histogram here:**
This is a histogram showing the distribution of ticket fares on the Titanic. Here's a detailed description:

X-axis (Fare): Represents the fare passengers paid, divided into bins of 10 units each (0–10, 10–20, …, up to 150).

Y-axis (Frequency): Shows the number of passengers whose fare falls within each bin.

Distribution shape: Highly right-skewed – most passengers paid lower fares (0–50 range), and very few passengers paid high fares (above 100).

Peak: The highest bar is in the 0–10 fare range, indicating that the majority of passengers bought cheaper tickets.

Trend: The frequency decreases steadily as the fare increases, with a few outliers who paid very high fares.

Overall, the histogram shows that ticket prices were mostly low, with a small number of passengers paying very high amounts.


"""

# ============================================================
# HISTOGRAMS
# ============================================================

bin_edges = [0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]
plt.hist(df['Fare'], histtype='bar', edgecolor='black', bins=bin_edges)

plt.xlabel('Fare')
plt.ylabel('Frequency')
plt.title('Fare of Ticket on the Titanic')
plt.show()

"""<font color="red">**Question 3: Compare with boxplots between the Age of the passenger and whether he or she Survived (0 means did not survive and 1 survived).**</font>

**Write your comments on the boxplots here:**
This is a boxplot comparing passenger age by survival status from a dataset (likely the Titanic dataset). Here's what it shows:

X-axis (Survived):

0 = Did not survive

1 = Survived

Y-axis (Age): Age of passengers.

Boxplot interpretation:

The box represents the interquartile range (IQR), i.e., the middle 50% of ages.

The line inside the box is the median age.

The whiskers extend to roughly 1.5 × IQR from the quartiles.

Circles above the whiskers are outliers, passengers with ages unusually high compared to the rest.

Observations from the plot:

Median age is slightly lower for survivors (1) than non-survivors (0).

The age distribution is fairly similar between survivors and non-survivors.

There are some older passengers as outliers in both groups, e.g., one passenger over 80 who did not survive and one over 60 who survived.

Most passengers were between ~20 and ~40 years old.

"""

# ============================================================
# boxplot
# ============================================================

sns.boxplot(x="Survived", y="Age", data=df)
plt.xlabel("Survived (0 = No, 1 = Yes)")
plt.ylabel("Age")
plt.title("Comparison of Passenger Age by Survival Status")
plt.show()

"""<font color="red">**Question4: Draw a scatterplot to find the relation between the Age of the passenger and the Fare he/she paid. Use the hue function to have the Survived in the graph**</font>

**Write your comments here:**
X-axis (Age): Passenger age, ranging roughly from 0 to 80 years.

Y-axis (Fare): Ticket fare paid by the passenger, ranging from 0 to above 500.

Color (Survived):

Blue points = passengers who did not survive

Orange points = passengers who survived

Observations:

Most passengers paid fares below 100, regardless of survival.

Survival seems scattered across age, but a slightly higher concentration of survivors appears in younger age groups (0–40).

There are a few extreme fare outliers (e.g., above 500), all of which are survivors.

There is no clear linear relationship between age and fare, but higher fares may have slightly higher survival rates.

"""

# ============================================================
#  SCATTERPLOT
# ============================================================

# Example: Age vs Fare colored by Survival
sns.lmplot(data=df, x='Age', y='Fare', hue='Survived', fit_reg=False)
plt.title('Age vs Fare by Survival')
plt.xlabel('Age')
plt.ylabel('Fare')
plt.show()

"""<font color="red">**Question 5: Draw a bar graph for the Passenger class on the titanic (Pclass) using seabornand and compare it to whether the passenger survived or not using the hue function**</font>

**Write your comments here:**
The code uses Seaborn’s countplot() to display the relationship between Passenger Class (Pclass) and Survival (Survived).

The hue='Survived' parameter separates passengers based on survival status (0 = did not survive, 1 = survived).

The palette='Set2' adds distinct colors for better visual clarity.

The x-axis represents passenger classes (1st, 2nd, 3rd).

The y-axis represents the number of passengers (Count).

Title: “Survival Count by Passenger Class”.

Observation:

1st class: More passengers survived than died.

2nd class: Survival numbers are slightly higher than deaths.

3rd class: Most passengers were in this class, but most did not survive.

Conclusion: Passengers in higher classes had a greater chance of survival compared to those in lower classes.

"""

# ============================================================
# BAR GRAPH
# ============================================================

sns.countplot(data=df, x='Pclass', hue='Survived', palette='Set2')
plt.title('Survival Count by Passenger Class')
plt.xlabel('Passenger Class')
plt.ylabel('Count')
plt.show()

"""<font color="red">**Question 6: Run the following cell to draw a heatmap for the numeric values in the titanic dataset**</font>

**Write your comments about the heatmap here**:
The code generates a heatmap of correlations using Seaborn and Matplotlib to visualize relationships between numerical columns in a dataset (df).

The df.corr(numeric_only=True) function computes correlation coefficients for numeric variables only.

The annot=True option displays the correlation values within each cell.

The cmap='coolwarm' color map shows positive correlations in red and negative correlations in blue.

The title of the plot is “Correlation Heatmap”.

Interpretation of the Heatmap:

Survived vs Pclass (-0.34): Negative correlation — passengers in higher classes (lower class number) were more likely to survive.

Pclass vs Fare (-0.55): Strong negative correlation — higher-class passengers paid higher fares.

Age vs Pclass (-0.37): Negative correlation — first-class passengers tended to be older.

SibSp vs Parch (0.41): Moderate positive correlation — passengers traveling with siblings/spouses often also had parents/children with them.

Fare vs Survived (0.26): Slight positive correlation — passengers who paid higher fares had better survival chances.

Diagonal values (1.0): Each variable’s perfect correlation with itself.
"""

# ============================================================
# HEATMAP OF CORRELATIONS
# ============================================================

plt.figure(figsize=(10,6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""# **Section 2: Data Cleaning and Preprocessing**

<font color="red">**Question 1: Check if there is any missing data in the titanic dataset**</font>

**Write your answer here:**
The code checks for missing data in the dataset (df) using the isnull().sum() function.

This command counts how many null (missing) values exist in each column.

The output displays the number of missing entries per column.

Interpretation of the Output:

Survived, Pclass, Sex, SibSp, Parch, Fare: No missing values (0).

Age: 177 missing values — a significant number, meaning age data is incomplete for many passengers.

Cabin: 687 missing values — the highest number, indicating most passengers don’t have recorded cabin information.

Embarked: 2 missing values — a small number of missing entries for port of embarkation.
"""

# ============================================================
#  CHECK FOR MISSING DATA
# ============================================================

print(df.isnull().sum())

"""<font color="red">**Question 2: Drop columns with more than 30% missing values.  Hint: check [Lab 2](https://moodle.bcu.ac.uk/mod/folder/view.php?id=8931483) for help on this**</font>

The code calculates the percentage of missing data in the dataset (df) using isnull().sum() divided by the total number of rows, then multiplies by 100 to get a percentage.

This helps identify columns with a high proportion of missing values. Columns with more than 30% missing data are considered unreliable and are dropped from the dataset using df.drop().

Interpretation of the Output:

Survived, Pclass, Sex, SibSp, Parch, Fare: 0% missing — complete data.

Age: 19.87% missing — some missing entries, may require imputation.

Cabin: 77.10% missing — very high missing rate, so it is dropped.

Embarked: 0.22% missing — very few missing entries, can be imputed or ignored.

Dropped Columns: ['Cabin']
Remaining Columns: ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
"""

# ============================================================
# DROP COLUMNS WITH >30% MISSING VALUES
# ============================================================

# You can get the percentage of missing values in each column
missing_pct = (df.isnull().sum() / len(df)) * 100
print(missing_pct)

# Columns with more than 30% missing values
drop_cols = missing_pct[missing_pct > 30].index

# Drop these columns
df = df.drop(columns=drop_cols)

print("\nDropped columns:", list(drop_cols))
print("\nRemaining columns:", df.columns)

"""Purpose of the code: Check for missing values in each column of the dataset df.

Method used: df.isnull().sum() counts the number of null (missing) values per column.

Output interpretation:

Survived, Pclass, Sex, SibSp, Parch, Fare: 0 missing values — complete data.

Age: 177 missing values — some data missing, may require imputation.

Embarked: 2 missing values — very few missing entries, can be easily handled.
"""

# ============================================================
#  CHECK FOR MISSING DATA
# ============================================================

print(df.isnull().sum())

"""<font color="red">**Question 3: Fill any other missing values with the appropriate method. You should still have Age and Embarked values with missing points. Go to Lab2 to see how filled  in missing data**</font>

Purpose: Handle missing values in the dataset for Age and Embarked.


Fill Age: Missing Age values are replaced with the median, which is robust to skewed distributions.


Fill Embarked: Missing Embarked values are replaced with the mode (most frequent value).


Check Age distribution:


Skewness: 0.51 — slight positive skew, meaning most ages are lower but a few higher ages pull the mean up.


Outliers (99th percentile): 65 — the top 1% of ages are 65 or higher, highlighting potential outliers.
"""

# ============================================================
# FILL MISSING VALUES
# ============================================================

# Fill missing Age values with the median (since Age is numeric and can be skewed)
df['Age'] = df['Age'].fillna(df['Age'].median())

# Fill missing Embarked values with the mode (most common value)
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

# Check distribution of Age
print("Skewness:", df['Age'].skew())
print("Outliers (99th percentile):", df['Age'].quantile(0.99))

"""Purpose: Handle missing values in the dataset to prepare it for analysis or modeling.

Step 1: Calculate the mean of the Age column.

Step 2: Fill all missing Age values with the calculated mean using fillna().

Step 3: Determine the mode (most frequent value) of the Embarked column.

Step 4: Fill all missing Embarked values with the mode using fillna().

Step 5: Verify that there are no missing values left in Age and Embarked using isnull().sum().

Output Interpretation:

Age: 0 missing values — all missing ages filled with the mean.

Embarked: 0 missing values — all missing embarkation entries filled with the mode
"""

# ============================================================
# FILL MISSING VALUES
# ============================================================

# Fill missing Age values with the mean age
mean_age = df['Age'].mean()
df['Age'] = df['Age'].fillna(mean_age)

# Fill missing Embarked values with the mode (most common value)
mode_embarked = df['Embarked'].mode()[0]
df['Embarked'] = df['Embarked'].fillna(mode_embarked)

print(df[['Age', 'Embarked']].isnull().sum())

"""Purpose: Fill missing values in the dataset to prepare it for analysis or modeling.

Age column:

Numeric column.

Missing values are filled with the median of the column to avoid skewing the distribution.

Embarked column:

Categorical column.

Missing values are filled with the mode (most frequent category) to maintain the most common value.

Verification: Prints the number of missing values after filling to ensure no nulls remain.

Output interpretation:

All columns now have 0 missing values.

Dataset is fully filled and ready for further processing.
"""

# ============================================================
# FILL MISSING VALUES
# ============================================================

# Fill missing Age values with the median (since Age is numeric)
df['Age'] = df['Age'].fillna(df['Age'].median())

# Fill missing Embarked values with the mode (most common category)
mode_Embarked = df['Embarked'].mode()[0]
df['Embarked'] = df['Embarked'].fillna(mode_Embarked)

print("Missing values after filling:\n", df.isnull().sum())

"""Purpose of the code: Checks if there are any remaining missing values in the dataset df.

Function used: df.isnull().sum() counts the number of missing (null) values in each column.

Interpretation of the output:

Survived: 0 missing values

Pclass: 0 missing values

Sex: 0 missing values

Age: 0 missing values

SibSp: 0 missing values

Parch: 0 missing values

Fare: 0 missing values

Embarked: 0 missing values

Conclusion: All columns now have complete data; there are no missing values remaining.
"""

#check if you still  have missing values
df.isnull().sum()

"""<font color="red">**Question 4: Scale the Fare variable with the StandardScaler()**</font>

Purpose: Standardize the Fare variable to have a mean of 0 and a standard deviation of 1.


Import: Uses StandardScaler from sklearn.preprocessing.


Select Variable: X = df[['Fare']] isolates the Fare column for scaling.


Define Scaler: scaler = StandardScaler() creates a scaler object.


Fit and Transform: scaledX = scaler.fit_transform(X) calculates the mean and standard deviation of Fare and scales the values.


Add to DataFrame: The scaled values are added as a new column Fare_scaled in the dataset.


Output Example: Shows the original Fare and corresponding standardized Fare_scaled values for the first five rows:


Row 0: 7.25 → -0.502


Row 1: 71.2833 → 0.787


Row 2: 7.925 → -0.489


Row 3: 53.1 → 0.421


Row 4: 8.05 → -0.486
"""

# ============================================================
# SCALE THE FARE VARIABLE
# ============================================================

from sklearn.preprocessing import StandardScaler

# Get the variable you want to scale
X = df[['Fare']]

# Define your scaler
scaler = StandardScaler()

# Transform your data
scaledX = scaler.fit_transform(X)

# (Optional) Add the scaled Fare back to the DataFrame
df['Fare_scaled'] = scaledX

print(df[['Fare', 'Fare_scaled']].head())

"""<font color="red">**Question 5: Impute the categorical data with the LabelEncoder:**</font>

<font color="red">**You will need to impute both the Sex and Embarked variable**</font>

Imports:

LabelEncoder from sklearn.preprocessing for encoding categorical variables.

pandas for handling the dataset (df).

Initialize Encoder:

le = LabelEncoder() creates an instance of the label encoder.

Encoding 'Sex' Column:

le.fit_transform(df['Sex']) converts the categorical values (male/female) into numeric labels (0 and 1).

The transformed values replace the original Sex column in the dataframe.

Encoding 'Embarked' Column:

le.fit_transform(df['Embarked']) converts the categorical port of embarkation (C, Q, S) into numeric labels (e.g., 0, 1, 2).

The transformed values replace the original Embarked column.

Output:

Shows the first 5 rows of the transformed Sex and Embarked columns.

Example:

Sex: 0 = female, 1 = male

Embarked: 0, 1, 2 represent different embarkation ports.

Result: Categorical features are now numeric, ready for machine learning models.
"""

from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Initialize encoder
le = LabelEncoder()

# Fit and transform the 'Sex' column
df['Sex'] = le.fit_transform(df['Sex'])

# Fit and transform the 'Embarked' column
df['Embarked'] = le.fit_transform(df['Embarked'])

print(df[['Sex', 'Embarked']].head())

"""# **Section 3: Model Training and Evaluation**

<font color="red">**Question 1: Divide your dataset into training and testing. You will be predicting whether the passenger survived or not from the Titanic based on the other variables**</font>

<font color="red">Hint check [Lab 7](https://moodle.bcu.ac.uk/mod/folder/view.php?id=9197859) for help on this section</font>

Purpose: Define the input features and target variable, then split the dataset into training and testing sets.

Features (X): All columns except 'Survived'.

Target (y): The 'Survived' column.

Data Splitting: Uses train_test_split to divide the data:

80% for training (X_train, y_train)

20% for testing (X_test, y_test)

random_state=42 ensures reproducibility.

Output:

Training set size: (712, 8) — 712 samples, 8 features

Testing set size: (179, 8) — 179 samples, 8 features
Confirms the data has been successfully split and is ready for model training and evaluation.
"""

# ============================================================
#  DEFINE FEATURES AND TARGET
# ============================================================

from sklearn.model_selection import train_test_split

# Features (all columns except 'Survived')
X = df.drop('Survived', axis=1)

# Target variable
y = df['Survived']

# Split dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape)

"""Purpose: Split the dataset into training and testing sets for model building and evaluation.

Import: train_test_split from sklearn.model_selection.

Features (X): All columns except Survived.

Target (y): The Survived column.

Split ratio: 80% of data for training, 20% for testing.

Random state: Set to 42 for reproducibility.

Output:Training set size: (712, 8)

Contains 712 rows (passengers) used to train the model.

Contains 8 columns (all features except the target Survived).

Testing set size: (179, 8)

Contains 179 rows reserved for evaluating the model’s performance.

Also has 8 columns (same features as the training set).

Interpretation: About 80% of the dataset is used for learning patterns, while 20% is held out to test how well the model generalizes to unseen data. This ensures the model is not overfitting to the training data.
"""

# ============================================================
# TRAIN-TEST SPLIT
# ============================================================

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = df.drop('Survived', axis=1)  # all columns except the target
y = df['Survived']               # target variable

# Split the dataset: 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape)

"""<font color="red">**Question 2: Train a Logistic Regression model for survival prediction**</font>

Purpose: Train a logistic regression model to predict a target variable.

Import: LogisticRegression from sklearn.linear_model.

Model Initialization:

max_iter=1000 ensures the algorithm has enough iterations to converge.

Model Training:

model.fit(X_train, y_train) fits the logistic regression model on the training data.

Optional Step:

model.score(X_train, y_train) computes the training accuracy.

Output:

Training Accuracy: 0.8006 (~80% of training instances are correctly predicted).
"""

# ============================================================
# TRAIN LOGISTIC REGRESSION MODEL
# ============================================================

from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression(max_iter=1000)  # increase max_iter in case of convergence issues

# Train the model
model.fit(X_train, y_train)

# (Optional) Check training accuracy
train_accuracy = model.score(X_train, y_train)
print("Training Accuracy:", train_accuracy)

"""<font color="red">**Question 3: Predict the X_text and evaluate predictions of the model against the true labels**</font>

<font color="red">**Explain the classification reprot here:**</font>

urpose: Evaluate the performance of a trained classification model on the test set.


Step 1: Import classification_report from sklearn.metrics.


Step 2: Use the model to predict labels for the test features (X_test) and store them in y_pred.


Step 3: Generate and print a detailed classification report comparing predicted labels (y_pred) with true labels (y_test).


Interpretation of the Output:


Class 0 (e.g., did not survive):


Precision: 0.83 — 83% of predicted class 0 were correct.


Recall: 0.86 — 86% of actual class 0 were correctly identified.


F1-score: 0.84 — harmonic mean of precision and recall.


Support: 105 — number of actual samples in class 0.




Class 1 (e.g., survived):


Precision: 0.79 — 79% of predicted class 1 were correct.


Recall: 0.74 — 74% of actual class 1 were correctly identified.


F1-score: 0.76 — harmonic mean of precision and recall.


Support: 74 — number of actual samples in class 1.




Overall metrics:


Accuracy: 0.81 — 81% of all predictions were correct.


Macro average: Precision 0.81, Recall 0.80, F1-score 0.80 — unweighted average across classes.


Weighted average: Precision 0.81, Recall 0.81, F1-score 0.81 — averages weighted by the number of samples in each class.
"""

# ============================================================
# EVALUATION
# ============================================================

from sklearn.metrics import classification_report

# Predict on the test set
y_pred = model.predict(X_test)

# Print classification report
print("Classification Report:\n")
print(classification_report(y_test, y_pred))

"""<font color="red">**Question 4: Run the following cell and explain the confusion matrix here:**</font>

The model correctly predicted 90 passengers as Not Survived (true negatives).
• It correctly predicted 55 passengers as Survived (true positives).
• It incorrectly predicted 15 passengers as Survived when they actually did not survive (false positives).
• It incorrectly predicted 19 passengers as Not Survived when they actually survived (false negatives).
• Overall, the model is slightly better at recognizing passengers who did not survive compared to those who survived.
"""

# ============================================================
# CONFUSION MATRIX GRAPH
# ============================================================

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Survived', 'Survived'])
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

"""**Optional: For extra marks try to improve the performance of the model. Explain your methods here:**"""